{
  "task_type": "llm",
  "input_dim": 512,
  "hidden_dim": 512,
  "output_dim": 100277,
  "liquid_units": 256,
  "liquid_backbone": "ltc",
  "spiking_units": 128,
  "spike_threshold": 0.8,
  "beta": 0.9,
  "num_spike_steps": 32,
  "num_layers": 8,
  "num_attention_heads": 8,
  "embedding_dim": 512,
  "max_position_embeddings": 1024,
  "vocab_size": 100277,
  "conv_channels": null,
  "conv_kernel_sizes": null,
  "conv_strides": null,
  "conv_padding": null,
  "dropout": 0.15,
  "attention_dropout": 0.15,
  "embedding_dropout": 0.15,
  "sequence_length": 512,
  "batch_size": 6,
  "learning_rate": 0.0002,
  "weight_decay": 0.02,
  "gradient_clip": 1.5,
  "mixed_precision": true,
  "device": "cuda",
  "seed": 123,
  "num_epochs": 12,
  "layer_norm_eps": 1e-05,
  "initializer_range": 0.02,
  "use_cache": true
}