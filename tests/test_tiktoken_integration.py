#!/usr/bin/env python3

import sys
sys.path.append('src')

from src.core.tokenizer_upgrade import AdvancedTokenizerManager

def test_tiktoken_integration():
    print("ğŸ§ª Testing TikToken GPT-3/GPT-4 Integration")
    print("=" * 60)
    
    # Test all tokenizers
    tokenizers = ["gpt2", "gpt3", "gpt4", "o200k"]
    
    for tokenizer_type in tokenizers:
        print(f"\nğŸ”¤ Testing {tokenizer_type.upper()} tokenizer:")
        try:
            manager = AdvancedTokenizerManager(tokenizer_type=tokenizer_type)
            tokenizer = manager.tokenizer
            
            print(f"   âœ… Loaded successfully")
            print(f"   ğŸ“š Vocab size: {tokenizer.vocab_size:,}")
            
            # Test encoding/decoding
            test_text = "Hello, world! This is a test of the tiktoken integration."
            tokens = tokenizer.encode(test_text)
            decoded = tokenizer.decode(tokens)
            
            print(f"   ğŸ”¢ Tokens ({len(tokens)}): {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
            print(f"   ğŸ”„ Round-trip: {'âœ…' if decoded.strip() == test_text.strip() else 'âŒ'}")
            
            # Validate token range
            valid_tokens = all(0 <= t < tokenizer.vocab_size for t in tokens)
            print(f"   ğŸ¯ Token range valid: {'âœ…' if valid_tokens else 'âŒ'}")
            
        except Exception as e:
            print(f"   âŒ Error: {e}")
    
    print(f"\nğŸ‰ TikToken integration test complete!")
    print("\nğŸ“‹ Summary:")
    print("   â€¢ TikTokenWrapper: HuggingFace-compatible wrapper")
    print("   â€¢ AdvancedTokenizerManager: Multi-tokenizer support")
    print("   â€¢ DatasetFactory: Enhanced with tokenizer_type parameter")
    print("   â€¢ CLI Integration: --tokenizer argument support")
    print("   â€¢ Training: Corrected vocab sizes for all tokenizers")

if __name__ == "__main__":
    test_tiktoken_integration()